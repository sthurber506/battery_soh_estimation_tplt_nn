{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Script...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "import cupy as cp\n",
    "import cudf\n",
    "import numpy as np\n",
    "import tables\n",
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "import gc\n",
    "import itertools\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import datetime\n",
    "import dateutil\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clear the GPU memory\n",
    "with cp.cuda.Device(0):\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "# Check if there is a 'df' variable in the environment and delete it\n",
    "if 'df' in locals():\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "print('Starting Script...')\n",
    "\n",
    "\n",
    "# Set the path to the folder where the csv file is located\n",
    "# path_folder_csv = Path('/mnt/shared/real_single_cell_data')\n",
    "path_folder_csv = Path('/mnt/c/Users/sthur/OneDrive/Documents/School Graduate 2024 Spring/CSC_561_Machine_Learning/semester_project/project_files/lg_cell_data/single_cell_data/data_for_NN')\n",
    "\n",
    "# Set the file name of the csv to load\n",
    "# file_name = 'dataq_07-16-2024-13-20-00_TPLT_0.2C_0.02steps.csv'\n",
    "file_name = 'cell_01_TPLT_0p2C_3139Ah.csv'\n",
    "\n",
    "# Full path to the csv file\n",
    "path_file_csv = path_folder_csv / file_name\n",
    "\n",
    "# Define the column names for the csv file\n",
    "column_names = ['Time', 'Voltage', 'Current', 'Temperature', 'Temperature_Reference', 'Date', 'Clock_Time']\n",
    "\n",
    "# Load the csv file into a cudf dataframe, skipping the first 5 rows and using the column names defined above\n",
    "df = cudf.read_csv(path_file_csv, skiprows=5, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Interval: 0.039999999999054126\n",
      "Number of Rows for 10 Seconds: 250\n",
      "Number of Rows for 30 Minutes: 45000\n",
      "Number of TPLT Detected: 52\n"
     ]
    }
   ],
   "source": [
    "# Define the pulse parameters to look for in the data\n",
    "dV_threshold = -0.01 # Looking for decrease in voltage of this magnitude to qualify as starting point of a pulse\n",
    "dI_threshold = 0.1 # Looking for increase in current of this magnitude to qualify as starting point of a pulse\n",
    "load_pulse_threshold = 0.2 # Looking for a value from the 'Current' column that is greater than this threshold to qualify as a load pulse\n",
    "load_pause_threshold = 0.0 # Looking for a value from the 'Current' column that is less than or equal to this threshold to qualify as a load pause\n",
    "load_pulse_true_value = -0.6191 # The true value of 'Current' in Amps from the B&K Precision Electronic Load that indicates a load pulse\n",
    "Capacity_initialized = 3.139 # The capacity of the battery in Amp-hours obtained from a full charge and discharge cycle\n",
    "AHC_nominal = 3.139 # The nominal capacity of the battery in Amp-hours\n",
    "C_rate_initialized = 0.2 # The C-rate of the battery during the test based on the nominal capacity of the battery\n",
    "\n",
    "# Create a function that will be used to sort cudf dataframes by the 'Time' column\n",
    "def sort_df(df):\n",
    "    return df.sort_values(by='Time')\n",
    "\n",
    "# Sort the dataframe by the 'Time' column using the function defined above\n",
    "df = sort_df(df)\n",
    "\n",
    "# Calculate the sampling interval by taking the difference between the first two 'Time' values\n",
    "sampling_interval = df['Time'].iloc[1] - df['Time'].iloc[0]\n",
    "print(f'Sampling Interval: {sampling_interval}')\n",
    "\n",
    "# Calculate the number of rows it takes for 10 seconds to pass at the sampling interval and store it in a variable called 'num_rows_10_seconds'\n",
    "num_rows_10_seconds = int(10 / sampling_interval)\n",
    "print(f'Number of Rows for 10 Seconds: {num_rows_10_seconds}')\n",
    "\n",
    "# Calculate the number of rows it takes for 30 minutes to pass at the sampling interval and store it in a variable called 'num_rows_30_minutes'\n",
    "num_rows_30_minutes = int((30 * 60) / sampling_interval)\n",
    "print(f'Number of Rows for 30 Minutes: {num_rows_30_minutes}')\n",
    "\n",
    "# Store a tolerance value for the number of additional rows to look past the 10 second window\n",
    "num_rows_tolerance = 20\n",
    "\n",
    "# Sort the dataframe by the 'Time' column using the function defined above\n",
    "df = sort_df(df)\n",
    "\n",
    "\n",
    "\n",
    "# First, create a 'SOH' column by dividing the 'Capacity_initialized' value by the 'AHC_nominal' value\n",
    "df['SOH'] = Capacity_initialized / AHC_nominal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The values in the 'Current' column are voltage values from the current sensor, which are not accurate.\n",
    "# Instead of converting the values, I know the true current values of the pulses from the B&K Precision electronic load.\n",
    "# So for all data in the 'Current' column, if the value is <= load_pulse_threshold, replace it with 0.0, otherwise replace it with load_pulse_true_value.\n",
    "# This will give the true current values for the pulses.\n",
    "df['Current'] = cp.where(df['Current'] <= load_pulse_threshold, 0.0, load_pulse_true_value)\n",
    "df = sort_df(df)\n",
    "\n",
    "\n",
    "# Calculate the instantaneous Ah column by multiplying the 'Current' column by the sampling interval, then dividing by 3600 to convert from seconds to hours\n",
    "df['Ah_instantaneous'] = df['Current'] * (sampling_interval / 3600)\n",
    "df = sort_df(df)\n",
    "\n",
    "# Calculate the cumulative Ah column by taking the cumulative sum of the 'Ah_instantaneous' column\n",
    "df['Ah'] = df['Ah_instantaneous'].cumsum()\n",
    "df = sort_df(df)\n",
    "\n",
    "# Create a new column called 'Ah_remaining' which is the difference between the 'Ah' column and the 'Capacity_initialized' value\n",
    "df['Ah_remaining'] = Capacity_initialized + df['Ah']\n",
    "df = sort_df(df)\n",
    "\n",
    "# Create a new column called 'SOC' which is the 'Ah_remaining' column divided by the 'Capacity_initialized' value\n",
    "df['SOC'] = df['Ah_remaining'] / Capacity_initialized\n",
    "df = sort_df(df)\n",
    "\n",
    "# Create a new column called 'Capacity_initialized' which is the 'Capacity_initialized' value\n",
    "df['Capacity_initialized'] = Capacity_initialized\n",
    "df = sort_df(df)\n",
    "\n",
    "# Create a new column called 'C_rate_initialized' which is the 'C_rate_initialized' value\n",
    "df['C_rate_initialized'] = C_rate_initialized\n",
    "df = sort_df(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Choose a number of rows to search above and below any non-zero 'Current' value to determine if it is a false positive\n",
    "num_rows_search = 5\n",
    "\n",
    "# Add a column to the dataframe called 'Current_shifted_down' which is the 'Current' column shifted down by num_rows_search rows\n",
    "df['Current_shifted_down'] = df['Current'].shift(num_rows_search)\n",
    "df = sort_df(df)\n",
    "\n",
    "# Add a column to the dataframe called 'Current_shifted_up' which is the 'Current' column shifted up by num_rows_search\n",
    "df['Current_shifted_up'] = df['Current'].shift(-num_rows_search)\n",
    "df = sort_df(df)\n",
    "\n",
    "\n",
    "# Look through the 'Current' column for non-zero values that have a corresponding 'Current_shifted_down' AND 'Current_shifted_up' that are both zero, \n",
    "# If these conditions are met, then set the 'Current' value for that row to 0.0.\n",
    "filtered_df = df.query('(Current != 0) & (Current_shifted_down == 0) & (Current_shifted_up == 0)').assign(Current=0.0)\n",
    "\n",
    "# Rename the 'Current' column in the filtered dataframe to 'Current_filtered'\n",
    "filtered_df = filtered_df.rename(columns={'Current': 'Current_filtered'})\n",
    "\n",
    "# Merge the 'filtered_df' dataframe with the original 'df' dataframe on the common columns.\n",
    "# This operation adds the 'Current_filtered' column to the original 'df' dataframe, aligning each filtered current value with the corresponding row.\n",
    "# The merge is done with a 'left' join to ensure all rows from the original 'df' dataframe are retained, even if they don't have a corresponding 'Current_filtered' value.\n",
    "common_columns = ['Time', 'Voltage', 'Temperature', 'Temperature_Reference', 'Date', 'Clock_Time']\n",
    "df = df.merge(filtered_df[common_columns + ['Current_filtered']], on=common_columns, how='left')\n",
    "\n",
    "# Replace the 'Current' column in 'df' with 'Current_filtered' where it exists\n",
    "df['Current'] = df['Current_filtered'].fillna(df['Current'])\n",
    "\n",
    "# Drop the 'Current_filtered' column as it's no longer needed\n",
    "df = df.drop(columns=['Current_filtered'])\n",
    "\n",
    "# Sort the dataframe by the 'Time' column using the function defined above\n",
    "df = sort_df(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add a column to the dataframe called 'dI' which is the difference between the current row and the previous row in the 'Current' column\n",
    "df['dI'] = df['Current'].diff()\n",
    "df = sort_df(df)\n",
    "\n",
    "# Add a column to the dataframe called 'dI_shifted_down' which is the 'dI' column shifted down by 1 row\n",
    "df['dI_shifted_down'] = df['dI'].shift(1)\n",
    "df = sort_df(df)\n",
    "\n",
    "# Add a column called 'I_shifted_up_10_seconds' which is the 'Current' column shifted up by the number of rows it takes for 10 seconds to pass + the tolerance value\n",
    "df['I_shifted_up_10_seconds'] = df['Current'].shift(-(num_rows_10_seconds + num_rows_tolerance))\n",
    "df = sort_df(df)\n",
    "\n",
    "# Add a column called 'I_shifted_up_20_seconds' which is the 'Current' column shifted up by the number of rows it takes for 20 seconds to pass + the tolerance value\n",
    "df['I_shifted_up_20_seconds'] = df['Current'].shift(-2*(num_rows_10_seconds + num_rows_tolerance))\n",
    "df = sort_df(df)\n",
    "\n",
    "# Add a column called 'I_shifted_up_30_seconds' which is the 'Current' column shifted up by the number of rows it takes for 30 seconds to pass + the tolerance value\n",
    "df['I_shifted_up_30_seconds'] = df['Current'].shift(-3*(num_rows_10_seconds + num_rows_tolerance))\n",
    "df = sort_df(df)\n",
    "\n",
    "# Add a column called 'TPLT_detected' which returns True if the following conditions are met:\n",
    "# 1. The 'dI' column is NOT zero.\n",
    "# 2. The 'dI_shifted_down' column IS zero.\n",
    "# 3. The 'I_shifted_up_10_seconds' column IS zero.\n",
    "# 4. The 'I_shifted_up_20_seconds' column is NOT zero.\n",
    "# 5. The 'I_shifted_up_30_seconds' column IS zero.\n",
    "df['TPLT_detected'] = (df['dI'] != 0) & (df['dI_shifted_down'] == 0) & (df['I_shifted_up_10_seconds'] == 0) & (df['I_shifted_up_20_seconds'] != 0) & (df['I_shifted_up_30_seconds'] == 0)\n",
    "df = sort_df(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Count the number of TRUE values in the 'TPLT_detected' column and store it in a variable called 'num_TPLT_detected'\n",
    "num_TPLT_detected = df['TPLT_detected'].sum()\n",
    "print(f'Number of TPLT Detected: {num_TPLT_detected}')\n",
    "df = sort_df(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Grab the first 3 hours of data and save it to a csv file in the path_folder_csv directory\n",
    "# data_segment = num_rows_30_minutes*6\n",
    "# df = sort_df(df)\n",
    "# df.head(data_segment).to_csv(path_folder_csv / 'TPLT_data_segment.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Create a pandas dataframe from the cudf dataframe\n",
    "# df_pd = df.to_pandas()\n",
    "\n",
    "# # Identify the indices of rows where 'TPLT_detected' is True\n",
    "# tpl_detected_indices = df_pd.index[df_pd['TPLT_detected'] == True].tolist()\n",
    "\n",
    "# # Initialize an empty list to store the segments\n",
    "# segments = []\n",
    "\n",
    "# # For each index, grab 300 rows before and after that index\n",
    "# for idx in tpl_detected_indices:\n",
    "#     start_idx = max(0, idx - 300)\n",
    "#     end_idx = min(len(df_pd), idx + 300)\n",
    "#     segment = df_pd.iloc[start_idx:end_idx]\n",
    "#     segments.append(segment)\n",
    "\n",
    "# # Concatenate all segments into a single DataFrame\n",
    "# concatenated_segments = pd.concat(segments)\n",
    "\n",
    "# # Save the concatenated DataFrame to a CSV file in the path_folder_csv directory\n",
    "# concatenated_segments.to_csv(path_folder_csv / 'TPLT_segments.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TPLT groups: 52\n"
     ]
    }
   ],
   "source": [
    "# Extract a list of the indices where the TPLT events are detected and call it 'TPLT_event_indices'\n",
    "TPLT_event_indices = df[df['TPLT_detected'] == True].index.to_pandas().tolist()\n",
    "\n",
    "# Add a column called 'first_pulse_range' and initialize it with False values.\n",
    "df['first_pulse_range'] = False\n",
    "\n",
    "# Add a column called 'pause_range' and initialize it with False values.\n",
    "df['pause_range'] = False\n",
    "\n",
    "# Add a column called 'second_pulse_range' and initialize it with False values.\n",
    "df['second_pulse_range'] = False\n",
    "\n",
    "# Add a column called 'OCV_range' and initialize it with False values.\n",
    "df['OCV_range'] = False\n",
    "\n",
    "# Add a column called 'TPLT_group' to the cudf dataframe and initialize it with NaN values.\n",
    "df['TPLT_group'] = cp.nan\n",
    "\n",
    "# Create a function to get the data surrounding each TPLT event\n",
    "def extract_TPLT_data(df, TPLT_event_indices, num_rows_before, num_rows_for_OCV, num_rows_for_pulse):\n",
    "    TPLT_data_list = []\n",
    "\n",
    "    # Create an iterator variable to designate the group numbers\n",
    "    group_num = 0\n",
    "\n",
    "    # Create variables to designate the range of rows to search through for each range of data\n",
    "    buffer = 10\n",
    "    first_pulse_search_range_end = num_rows_for_pulse + buffer\n",
    "\n",
    "    pause_search_range_start = num_rows_for_pulse - buffer\n",
    "    pause_search_range_end = first_pulse_search_range_end + num_rows_for_pulse + buffer\n",
    "\n",
    "    second_pulse_search_range_start = pause_search_range_start + num_rows_for_pulse - buffer\n",
    "    second_pulse_search_range_end = pause_search_range_end + num_rows_for_pulse + buffer\n",
    "\n",
    "    OCV_search_range_start = second_pulse_search_range_start + num_rows_for_pulse - buffer\n",
    "    OCV_search_range_end = second_pulse_search_range_end + num_rows_for_OCV + buffer\n",
    "\n",
    "    for index in TPLT_event_indices:\n",
    "        # Use the range variables from above this for loop to set the start and end indices for each range of data\n",
    "        first_pulse_range_start = index - buffer\n",
    "        first_pulse_range_end = index + first_pulse_search_range_end\n",
    "\n",
    "        pause_range_start = index + pause_search_range_start\n",
    "        pause_range_end = index + pause_search_range_end\n",
    "\n",
    "        second_pulse_range_start = index + second_pulse_search_range_start\n",
    "        second_pulse_range_end = index + second_pulse_search_range_end\n",
    "\n",
    "        OCV_range_start = index + OCV_search_range_start\n",
    "        OCV_range_end = index + OCV_search_range_end\n",
    "\n",
    "        # First, set the absolute starting point for the range of data that will be extracted\n",
    "        start_idx = max(index - num_rows_before, 0)\n",
    "\n",
    "        # Next, search the first pulse range for any rows that have a 'Current' value that is NOT equal to zero, and set the 'first_pulse_range' to True for those rows.\n",
    "        first_pulse_non_zero = df.loc[first_pulse_range_start:first_pulse_range_end, 'Current'] != 0\n",
    "        if first_pulse_non_zero.any():\n",
    "            df.loc[first_pulse_range_start:first_pulse_range_end, 'first_pulse_range'] = first_pulse_non_zero\n",
    "            \n",
    "            \n",
    "\n",
    "        # Next, search the pause range for any rows that have a 'Current' value that IS equal to zero, and set the 'pause_range' to True for those rows.\n",
    "        pause_zero = df.loc[pause_range_start:pause_range_end, 'Current'] == 0\n",
    "        if pause_zero.any():\n",
    "            df.loc[pause_range_start:pause_range_end, 'pause_range'] = pause_zero\n",
    "            \n",
    "\n",
    "        # Next, search the second pulse range for any rows that have a 'Current' value that is NOT equal to zero, and set the 'second_pulse_range' to True for those rows.\n",
    "        second_pulse_non_zero = df.loc[second_pulse_range_start:second_pulse_range_end, 'Current'] != 0\n",
    "        if second_pulse_non_zero.any():\n",
    "            df.loc[second_pulse_range_start:second_pulse_range_end, 'second_pulse_range'] = second_pulse_non_zero\n",
    "            \n",
    "\n",
    "        # Next, search the OCV range for any rows that have a 'Current' value that IS equal to zero, and set the 'OCV_range' to True for those rows.\n",
    "        OCV_zero = df.loc[OCV_range_start:OCV_range_end, 'Current'] == 0\n",
    "        if OCV_zero.any():\n",
    "            df.loc[OCV_range_start:OCV_range_end, 'OCV_range'] = OCV_zero\n",
    "            \n",
    "\n",
    "        # Now search the OCV range again, and find the last row that had 'OCV_range' set to True, and set the end index to that row.\n",
    "        OCV_true_indices = df.loc[OCV_range_start:OCV_range_end].query('OCV_range == True').index\n",
    "        if not OCV_true_indices.empty:\n",
    "            end_idx = OCV_true_indices[-1]\n",
    "        else:\n",
    "            end_idx = OCV_range_end\n",
    "\n",
    "        # Set the 'TPLT_group' column to the group number for the range of data extracted\n",
    "        df.loc[start_idx:end_idx, 'TPLT_group'] = group_num\n",
    "\n",
    "        # Increment the group number\n",
    "        group_num += 1\n",
    "\n",
    "        # Append the data from the start index to the end index to the TPLT_data_list\n",
    "        TPLT_data_list.append(df[start_idx:end_idx])\n",
    "\n",
    "\n",
    "    return cudf.concat(TPLT_data_list)\n",
    "\n",
    "# Extract the TPLT data for each TPLT event with 10 seconds before and 30 minutes after\n",
    "TPLT_data = extract_TPLT_data(df, TPLT_event_indices, num_rows_10_seconds, num_rows_30_minutes, num_rows_10_seconds)\n",
    "\n",
    "# Print the number of TPLT_groups\n",
    "print(f'Number of TPLT groups: {len(TPLT_data[\"TPLT_group\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we will add a new column called 'TPLT_Vmax' and initialize it with NaN values, then we will group the TPLT data by the 'TPLT_group' column and do the following for each group:\n",
    "# 1. Find the data for the pause between the pulses.\n",
    "# 2. Find the maximum voltage value in the range between the two pulses.\n",
    "# 3. Replace all NaN values in the group with the maximum voltage value found in step 2.\n",
    "# Recall that the dataframe is a cudf dataframe, so we will use the cudf groupby function to group the data, and the aggregation functions to calculate the required values.\n",
    "\n",
    "# First, query 'TPLT_data' to filter the rows where 'pause_range' is True and store it in a variable called 'pause_data'\n",
    "pause_data = TPLT_data.query(\"pause_range == True\")\n",
    "\n",
    "# Now, group the 'pause_data' dataframe by the 'TPLT_group' column, which will create a GroupBy object in cudf, which allows for aggregation operations to be performed on the groups.\n",
    "# Use the 'max' aggregation function to find the maximum voltage value in the 'Voltage' column for each group, and store the result in a variable called 'max_voltage_in_pause'.\n",
    "# Ensure to reset the index for merging later.\n",
    "max_voltage_in_pause = pause_data.groupby('TPLT_group').agg({'Voltage': 'max'}).reset_index()\n",
    "\n",
    "# Rename the 'Voltage' column to 'TPLT_Vmax' in the 'max_voltage_in_pause' dataframe to clearly indicate that this column represents the maximum voltage value for each group.\n",
    "# This step makes the DataFrame ready for merging with the original TPLT data by aligning the column names.\n",
    "max_voltage_in_pause = max_voltage_in_pause.rename(columns={'Voltage': 'TPLT_Vmax'})\n",
    "\n",
    "# Merge the 'max_voltage_in_pause' dataframe with the original 'TPLT_data' dataframe on the 'TPLT_group' column.\n",
    "# This operation adds the 'TPLT_Vmax' column to the original 'TPLT_data' dataframe, aligning each 'TPLT_Vmax' value with the corresponding group.\n",
    "# The merge is done with a 'left' join to ensure all rows from the original 'TPLT_data' dataframe are retained, even if they don't have a corresponding 'TPLT_Vmax' value.\n",
    "TPLT_data = TPLT_data.merge(max_voltage_in_pause, on='TPLT_group', how='left')\n",
    "\n",
    "# Fill NaN values in the 'TPLT_Vmax' column. This step is necessary because the first row of the DataFrame and \n",
    "# any groups that did not meet the filtering criteria will have NaN values for 'TPLT_Vmax'. \n",
    "# Using 'ffill' (forward fill) propagates the last valid observation forward to next valid. \n",
    "# This assumes that the 'TPLT_Vmax' value of the first group is representative for any initial NaN values, providing a reasonable estimate for 'TPLT_Vmax' where the direct calculation was not applicable.\n",
    "TPLT_data['TPLT_Vmax'] = TPLT_data['TPLT_Vmax'].ffill()\n",
    "\n",
    "# Sort the 'TPLT_data' dataframe by the 'Time' column which should start with the lowest Time value and end with the highest Time value.\n",
    "TPLT_data = TPLT_data.sort_values(['Time'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Repeat the same process for creating the 'TPLT_Vmax' column, except now do it for a new column called 'TPLT_Vmin' which will store the minimum voltage value in the range of the 2nd pulse.\n",
    "# Query 'TPLT_data' to filter the rows where 'second_pulse_range' is True and store it in a variable called 'second_pulse_data'\n",
    "second_pulse_data = TPLT_data.query(\"second_pulse_range == True\")\n",
    "\n",
    "# Group the 'second_pulse_data' dataframe by the 'TPLT_group' column and find the minimum voltage value in the 'Voltage' column for each group.\n",
    "# Store the result in a variable called 'min_voltage_in_second_pulse'.\n",
    "min_voltage_in_second_pulse = second_pulse_data.groupby('TPLT_group').agg({'Voltage': 'min'}).reset_index()\n",
    "\n",
    "# Rename the 'Voltage' column to 'TPLT_Vmin' in the 'min_voltage_in_second_pulse' dataframe.\n",
    "min_voltage_in_second_pulse = min_voltage_in_second_pulse.rename(columns={'Voltage': 'TPLT_Vmin'})\n",
    "\n",
    "# Merge the 'min_voltage_in_second_pulse' dataframe with the original 'TPLT_data' dataframe on the 'TPLT_group' column.\n",
    "TPLT_data = TPLT_data.merge(min_voltage_in_second_pulse, on='TPLT_group', how='left')\n",
    "\n",
    "# Fill NaN values in the 'TPLT_Vmin' column using 'ffill'.\n",
    "TPLT_data['TPLT_Vmin'] = TPLT_data['TPLT_Vmin'].ffill()\n",
    "\n",
    "# Sort the 'TPLT_data' dataframe by the 'Time' column.\n",
    "TPLT_data = TPLT_data.sort_values(['Time'])\n",
    "\n",
    "# Calculate the 'TPLT_delta_V2' column by subtracting the 'TPLT_Vmin' column from the 'TPLT_Vmax' column.\n",
    "TPLT_data['TPLT_delta_V2'] = TPLT_data['TPLT_Vmax'] - TPLT_data['TPLT_Vmin']\n",
    "\n",
    "# Sort the 'TPLT_data' dataframe by the 'Time' column.\n",
    "TPLT_data = TPLT_data.sort_values(['Time'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Repeat the same process for creating the 'TPLT_Vmax' column, except now do it for a new column called 'OCV' which will store the maximum voltage value in the range of the OCV.\n",
    "# Query 'TPLT_data' to filter the rows where 'OCV_range' is True and store it in a variable called 'OCV_data'\n",
    "OCV_data = TPLT_data.query(\"OCV_range == True\")\n",
    "\n",
    "# Group the 'OCV_data' dataframe by the 'TPLT_group' column and find the maximum voltage value in the 'Voltage' column for each group.\n",
    "# Store the result in a variable called 'max_voltage_in_OCV'.\n",
    "max_voltage_in_OCV = OCV_data.groupby('TPLT_group').agg({'Voltage': 'max'}).reset_index()\n",
    "\n",
    "# Rename the 'Voltage' column to 'OCV' in the 'max_voltage_in_OCV' dataframe.\n",
    "max_voltage_in_OCV = max_voltage_in_OCV.rename(columns={'Voltage': 'OCV'})\n",
    "\n",
    "# Merge the 'max_voltage_in_OCV' dataframe with the original 'TPLT_data' dataframe on the 'TPLT_group' column.\n",
    "TPLT_data = TPLT_data.merge(max_voltage_in_OCV, on='TPLT_group', how='left')\n",
    "\n",
    "# Fill NaN values in the 'OCV' column using 'ffill'.\n",
    "TPLT_data['OCV'] = TPLT_data['OCV'].ffill()\n",
    "\n",
    "# Sort the 'TPLT_data' dataframe by the 'Time' column.\n",
    "TPLT_data = TPLT_data.sort_values(['Time'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The 'C_rate_initialized' value is the C-rate of the battery during the test based on the nominal capacity of the battery.\n",
    "# The calculation for AHC is as follows:\n",
    "# AHC = I/C_rate\n",
    "# where I is the current in Amperes and C_rate is the C-rate.\n",
    "# Therefore, C_rate = I/AHC, and the C_rate_based_on_AHC_initialized column will be calculated using this formula,\n",
    "# where I is the 'Current' column and AHC is the 'Capacity_initialized' column.\n",
    "# Since all TPLT tests will only use one load value, we can use load_pulse_true_value for I, and Capacity_initialized for AHC.\n",
    "# Calculate the 'C_rate_based_on_AHC_initialized' column by dividing load_pulse_true_value by Capacity_initialized.\n",
    "# Use the absolute value of load_pulse_true_value since it is negative.\n",
    "TPLT_data['C_rate_based_on_AHC_initialized'] = cp.abs(load_pulse_true_value) / Capacity_initialized\n",
    "\n",
    "\n",
    "# Sort the 'TPLT_data' dataframe by the 'Time' column.\n",
    "TPLT_data = TPLT_data.sort_values(['Time'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find all groups that have SOC column values less than 0.12 and remove those groups from the dataframe.\n",
    "TPLT_data = TPLT_data.query('SOC >= 0.12')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Grab the data for the first two TPLT_groups and save it to a csv file in the path_folder_csv directory\n",
    "# TPLT_data.query(\"TPLT_group == 0 or TPLT_group == 1\").to_csv(path_folder_csv / 'TPLT_data_first_two_groups.csv')\n",
    "\n",
    "# # Calculate the maximum value of 'TPLT_group'\n",
    "# max_group = TPLT_data['TPLT_group'].max()\n",
    "\n",
    "# # Grab the data for the last two TPLT_groups and save it to a csv file in the path_folder_csv directory\n",
    "# TPLT_data.query(f\"TPLT_group == {max_group - 1} or TPLT_group == {max_group}\").to_csv(path_folder_csv / 'TPLT_data_last_two_groups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another cudf for exporting which has the data from the following columns of the TPLT_data dataframe in this order:\n",
    "# 'Time', 'Voltage', 'Current', 'SOC', 'Capacity_initialized', 'C_rate_initialized', 'OCV', 'TPLT_Vmax', 'TPLT_delta_V2', 'C_rate_based_on_AHC_initialized', 'SOH', 'TPLT_group'\n",
    "# Also, rename 'TPLT_Vmax' to 'tplt_V_max' and 'TPLT_delta_V2' to 'tplt_delta_V_2' and 'TPLT_group' to 'dataset_idx' to match the expected input format for the NN.\n",
    "TPLT_data_export = TPLT_data[['Time', 'Voltage', 'Current', 'SOC', 'Capacity_initialized', 'C_rate_initialized', 'OCV', 'TPLT_Vmax', 'TPLT_delta_V2', 'C_rate_based_on_AHC_initialized', 'SOH', 'TPLT_group']]\n",
    "TPLT_data_export = TPLT_data_export.rename(columns={'TPLT_Vmax': 'tplt_V_max', 'TPLT_delta_V2': 'tplt_delta_V_2', 'TPLT_group': 'dataset_idx'})\n",
    "\n",
    "# Save the TPLT_data_export cudf to a csv file in the path_folder_csv directory\n",
    "TPLT_data_export.to_csv(path_folder_csv / 'TPLT_data_export.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPLT Data Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'TPLT_data_export' cudf to a NumPy array and then to a PyTorch tensor\n",
    "TPLT_data_tensor = torch.tensor(TPLT_data_export.to_pandas().values, dtype=torch.float32)\n",
    "\n",
    "# Define the base output file name\n",
    "output_file_base = \"TPLT_data_tensor\"\n",
    "\n",
    "# Remove the '.csv' extension from file_name if it exists\n",
    "file_name = file_name.replace('.csv', '')\n",
    "\n",
    "# Append the `file_name` variable to the output file name\n",
    "output_file_name = f\"{output_file_base}_{file_name}.pt\"\n",
    "\n",
    "# Save the PyTorch tensor to a file using torch.save and save it to the path_folder_csv directory\n",
    "torch.save(TPLT_data_tensor, path_folder_csv / output_file_name)\n",
    "\n",
    "# Print a message indicating that the data has been saved\n",
    "print(f'TPLT Data Saved Successfully as {output_file_name}!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
